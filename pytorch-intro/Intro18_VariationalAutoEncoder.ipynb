{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "www.bigrabbitdata.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform  = transforms.Compose([transforms.ToTensor()\n",
    "                                ])\n",
    "training_dataset = datasets.MNIST(root='./mnist', train=True, \n",
    "                                  download=True, transform= transform)\n",
    "validation_dataset = datasets.MNIST(root='./mnist', train=False, \n",
    "                                  download=True, transform= transform) \n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, \n",
    "                                              batch_size=100,\n",
    "                                              shuffle=True)\n",
    "   \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                              batch_size=100,\n",
    "                                              shuffle=True)\n",
    "\n",
    "# Create directory to save results\n",
    "result_dir = 'Intro18-VAE-Result'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our Variational Autoencoder Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.fc2_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(h)\n",
    "        log_var = self.fc2_logvar(h)\n",
    "        return mean, log_var\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(logvar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc3(z))\n",
    "        out = torch.sigmoid(self.fc4(h))\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x.view(-1, self.input_size))\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VariationalAutoencoder(784, 400, 20).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction Loss +  KL Divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_image, original_image, mean, logvar):\n",
    "    bce = F.binary_cross_entropy(reconstructed_image, original_image.view(-1, 784), reduction = 'sum')\n",
    "    # kld = torch.sum(0.5 * torch.sum(logvar.exp() + mean.pow(2) - 1 - logvar, 1))\n",
    "    kld = 0.5 * torch.sum(logvar.exp() + mean.pow(2) - 1 - logvar)\n",
    "    return bce + kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 [Batch 100/600]\tLoss: 173.417\n",
      "Train Epoch 1 [Batch 200/600]\tLoss: 157.241\n",
      "Train Epoch 1 [Batch 300/600]\tLoss: 144.682\n",
      "Train Epoch 1 [Batch 400/600]\tLoss: 139.074\n",
      "Train Epoch 1 [Batch 500/600]\tLoss: 128.661\n",
      "Train Epoch 1 [Batch 600/600]\tLoss: 129.323\n",
      "=====> Epoch 1, Average Loss: 156.280\n",
      "Validation set: Average Loss: 123.136131\n",
      "Train Epoch 2 [Batch 100/600]\tLoss: 125.259\n",
      "Train Epoch 2 [Batch 200/600]\tLoss: 116.801\n",
      "Train Epoch 2 [Batch 300/600]\tLoss: 117.336\n",
      "Train Epoch 2 [Batch 400/600]\tLoss: 120.858\n",
      "Train Epoch 2 [Batch 500/600]\tLoss: 114.732\n",
      "Train Epoch 2 [Batch 600/600]\tLoss: 113.585\n",
      "=====> Epoch 2, Average Loss: 118.724\n",
      "Validation set: Average Loss: 113.701401\n",
      "Train Epoch 3 [Batch 100/600]\tLoss: 114.495\n",
      "Train Epoch 3 [Batch 200/600]\tLoss: 114.459\n",
      "Train Epoch 3 [Batch 300/600]\tLoss: 119.679\n",
      "Train Epoch 3 [Batch 400/600]\tLoss: 110.759\n",
      "Train Epoch 3 [Batch 500/600]\tLoss: 110.121\n",
      "Train Epoch 3 [Batch 600/600]\tLoss: 113.252\n",
      "=====> Epoch 3, Average Loss: 113.060\n",
      "Validation set: Average Loss: 110.637146\n",
      "Train Epoch 4 [Batch 100/600]\tLoss: 105.156\n",
      "Train Epoch 4 [Batch 200/600]\tLoss: 110.123\n",
      "Train Epoch 4 [Batch 300/600]\tLoss: 108.148\n",
      "Train Epoch 4 [Batch 400/600]\tLoss: 113.021\n",
      "Train Epoch 4 [Batch 500/600]\tLoss: 111.458\n",
      "Train Epoch 4 [Batch 600/600]\tLoss: 111.017\n",
      "=====> Epoch 4, Average Loss: 110.578\n",
      "Validation set: Average Loss: 108.788460\n",
      "Train Epoch 5 [Batch 100/600]\tLoss: 109.383\n",
      "Train Epoch 5 [Batch 200/600]\tLoss: 108.739\n",
      "Train Epoch 5 [Batch 300/600]\tLoss: 113.006\n",
      "Train Epoch 5 [Batch 400/600]\tLoss: 112.560\n",
      "Train Epoch 5 [Batch 500/600]\tLoss: 106.700\n",
      "Train Epoch 5 [Batch 600/600]\tLoss: 111.685\n",
      "=====> Epoch 5, Average Loss: 109.087\n",
      "Validation set: Average Loss: 107.774796\n",
      "Train Epoch 6 [Batch 100/600]\tLoss: 111.457\n",
      "Train Epoch 6 [Batch 200/600]\tLoss: 106.733\n",
      "Train Epoch 6 [Batch 300/600]\tLoss: 107.260\n",
      "Train Epoch 6 [Batch 400/600]\tLoss: 105.239\n",
      "Train Epoch 6 [Batch 500/600]\tLoss: 113.822\n",
      "Train Epoch 6 [Batch 600/600]\tLoss: 106.691\n",
      "=====> Epoch 6, Average Loss: 108.018\n",
      "Validation set: Average Loss: 107.287025\n",
      "Train Epoch 7 [Batch 100/600]\tLoss: 110.813\n",
      "Train Epoch 7 [Batch 200/600]\tLoss: 110.500\n",
      "Train Epoch 7 [Batch 300/600]\tLoss: 109.100\n",
      "Train Epoch 7 [Batch 400/600]\tLoss: 102.862\n",
      "Train Epoch 7 [Batch 500/600]\tLoss: 109.860\n",
      "Train Epoch 7 [Batch 600/600]\tLoss: 106.691\n",
      "=====> Epoch 7, Average Loss: 107.337\n",
      "Validation set: Average Loss: 106.744934\n",
      "Train Epoch 8 [Batch 100/600]\tLoss: 108.527\n",
      "Train Epoch 8 [Batch 200/600]\tLoss: 107.786\n",
      "Train Epoch 8 [Batch 300/600]\tLoss: 108.594\n",
      "Train Epoch 8 [Batch 400/600]\tLoss: 108.198\n",
      "Train Epoch 8 [Batch 500/600]\tLoss: 105.658\n",
      "Train Epoch 8 [Batch 600/600]\tLoss: 108.620\n",
      "=====> Epoch 8, Average Loss: 106.734\n",
      "Validation set: Average Loss: 105.935471\n",
      "Train Epoch 9 [Batch 100/600]\tLoss: 108.596\n",
      "Train Epoch 9 [Batch 200/600]\tLoss: 110.952\n",
      "Train Epoch 9 [Batch 300/600]\tLoss: 104.889\n",
      "Train Epoch 9 [Batch 400/600]\tLoss: 110.526\n",
      "Train Epoch 9 [Batch 500/600]\tLoss: 106.934\n",
      "Train Epoch 9 [Batch 600/600]\tLoss: 108.705\n",
      "=====> Epoch 9, Average Loss: 106.263\n",
      "Validation set: Average Loss: 105.403595\n",
      "Train Epoch 10 [Batch 100/600]\tLoss: 103.842\n",
      "Train Epoch 10 [Batch 200/600]\tLoss: 103.539\n",
      "Train Epoch 10 [Batch 300/600]\tLoss: 106.639\n",
      "Train Epoch 10 [Batch 400/600]\tLoss: 103.681\n",
      "Train Epoch 10 [Batch 500/600]\tLoss: 105.442\n",
      "Train Epoch 10 [Batch 600/600]\tLoss: 105.768\n",
      "=====> Epoch 10, Average Loss: 105.875\n",
      "Validation set: Average Loss: 105.243988\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# Main function\n",
    "for e in range(1, epochs + 1):\n",
    " \n",
    "    train_batch_loss = 0.0\n",
    "    train_epoch_loss = 0.0\n",
    "    val_epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx, data in enumerate(training_loader, start=1):\n",
    "        inputs = data[0].to(device)\n",
    "        reconstruct_image, mean, variance = model(inputs)\n",
    "        loss = loss_function(reconstruct_image, inputs, mean, variance)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_batch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Train Epoch {} [Batch {}/{}]\\tLoss: {:.3f}\".format(e, batch_idx, len(training_loader), loss.item()/len(inputs)))\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx, val_data in enumerate(validation_loader, start=1):\n",
    "\n",
    "                val_inputs = val_data[0].to(device)\n",
    "                val_reconstruct_image, val_mean, val_variance = model(val_inputs)\n",
    "                val_epoch_loss += loss_function(val_reconstruct_image, \n",
    "                                                val_inputs, \n",
    "                                                val_mean, \n",
    "                                                val_variance)\n",
    "                \n",
    "                if val_batch_idx  == 1:\n",
    "                    # Save one sample from each epoch\n",
    "                    comparison = torch.cat([val_inputs[:5], \n",
    "                                            val_reconstruct_image.view(-1, 1, 28, 28)[:5]])\n",
    "                    save_image(comparison.cpu(), result_dir + '/reconstruction_' + str(e) + '.png', nrow = 5)\n",
    "        \n",
    "        print('=====> Epoch {}, Average Loss: {:.3f}'.format(e, train_batch_loss/len(training_loader.dataset)))\n",
    "        print('Validation set: Average Loss: {:.6f}'.format(val_epoch_loss/len(validation_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 samples from gaussian distribution\n",
    "samples_latent_space = torch.randn(100, 20).to(device)\n",
    "generated_samples = model.decode(samples_latent_space).cpu()\n",
    "save_image(generated_samples.view(100, 1, 28, 28), \n",
    "           result_dir + '/generated_samples.png', \n",
    "           nrow=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
