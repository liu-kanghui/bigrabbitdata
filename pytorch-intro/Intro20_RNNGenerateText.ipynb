{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "www.bigrabbitdata.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping each unique word to an unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    '''\n",
    "        Create a dictionary, map each unique word to an unique index\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Text to get trainable input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcess(object):\n",
    "    '''\n",
    "        process the txt file and return batched-trainable data\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # add all the unique words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            total_tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                total_tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word) \n",
    "        \n",
    "        # Create a empty 1-D tensor that contains the index of all the words in the text file\n",
    "        input_tensor = torch.LongTensor(total_tokens)\n",
    "        index = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    input_tensor[index] = self.dictionary.word2idx[word]\n",
    "                    index += 1\n",
    "                    \n",
    "        # Find out how many batches we have          \n",
    "        num_batches = total_tokens // batch_size\n",
    "        \n",
    "        #Remove the leftover that doesn't fill a batch\n",
    "        input_tensor = input_tensor[:num_batches*batch_size]\n",
    "        # return (batch_size, size of each batch)\n",
    "        input_tensor = input_tensor.view(batch_size, -1)\n",
    "        return input_tensor, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 27226 total words in our text\n",
      "(batch_size, number of words for each batch) =  torch.Size([20, 1361])\n",
      "We have 4930 unique words in our dictionary\n",
      "number of batches 45\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128    # Input features to the LSTM\n",
    "hidden_size = 1024  # Number of LSTM units\n",
    "num_layers = 1      # Number of layer stacked  \n",
    "num_epochs = 50\n",
    "batch_size = 20\n",
    "timesteps = 30\n",
    "learning_rate = 0.002\n",
    "\n",
    "corpus = TextProcess()\n",
    "\n",
    "input_tensor, total_tokens = corpus.get_data('earnest.txt', batch_size)\n",
    "print (\"we have {} total words in our text\".format(total_tokens))\n",
    "\n",
    "print(\"(batch_size, number of words for each batch) = \", input_tensor.shape)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print (\"We have {} unique words in our dictionary\".format(vocab_size))\n",
    "\n",
    "num_batches = input_tensor.shape[1] // timesteps\n",
    "print(\"number of batches {}\".format(num_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embed(x)\n",
    "        output, (hn, cn) = self.lstm(embed, prev_state)\n",
    "        # output shape is (batch_size, timestep, hidden_size) = (20, 30, 1024)\n",
    "        \n",
    "        # reshape the output to match our fully connected layer\n",
    "        output = output.reshape(-1, output.size(2))\n",
    "        # (-1, 1024)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        return output, (hn, cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 6.4108\n",
      "Epoch [2/50], Loss: 5.4994\n",
      "Epoch [3/50], Loss: 4.7959\n",
      "Epoch [4/50], Loss: 4.1645\n",
      "Epoch [5/50], Loss: 3.6922\n",
      "Epoch [6/50], Loss: 3.1443\n",
      "Epoch [7/50], Loss: 2.7539\n",
      "Epoch [8/50], Loss: 2.4563\n",
      "Epoch [9/50], Loss: 2.0024\n",
      "Epoch [10/50], Loss: 1.6632\n",
      "Epoch [11/50], Loss: 1.3270\n",
      "Epoch [12/50], Loss: 0.9071\n",
      "Epoch [13/50], Loss: 0.6703\n",
      "Epoch [14/50], Loss: 0.4337\n",
      "Epoch [15/50], Loss: 0.2859\n",
      "Epoch [16/50], Loss: 0.1890\n",
      "Epoch [17/50], Loss: 0.1323\n",
      "Epoch [18/50], Loss: 0.0903\n",
      "Epoch [19/50], Loss: 0.0820\n",
      "Epoch [20/50], Loss: 0.0787\n",
      "Epoch [21/50], Loss: 0.0759\n",
      "Epoch [22/50], Loss: 0.0764\n",
      "Epoch [23/50], Loss: 0.0737\n",
      "Epoch [24/50], Loss: 0.0750\n",
      "Epoch [25/50], Loss: 0.0723\n",
      "Epoch [26/50], Loss: 0.0742\n",
      "Epoch [27/50], Loss: 0.0712\n",
      "Epoch [28/50], Loss: 0.0736\n",
      "Epoch [29/50], Loss: 0.0703\n",
      "Epoch [30/50], Loss: 0.0732\n",
      "Epoch [31/50], Loss: 0.0697\n",
      "Epoch [32/50], Loss: 0.0727\n",
      "Epoch [33/50], Loss: 0.0692\n",
      "Epoch [34/50], Loss: 0.0722\n",
      "Epoch [35/50], Loss: 0.0688\n",
      "Epoch [36/50], Loss: 0.0718\n",
      "Epoch [37/50], Loss: 0.0685\n",
      "Epoch [38/50], Loss: 0.0713\n",
      "Epoch [39/50], Loss: 0.0681\n",
      "Epoch [40/50], Loss: 0.0710\n",
      "Epoch [41/50], Loss: 0.0678\n",
      "Epoch [42/50], Loss: 0.0706\n",
      "Epoch [43/50], Loss: 0.0675\n",
      "Epoch [44/50], Loss: 0.0702\n",
      "Epoch [45/50], Loss: 0.0672\n",
      "Epoch [46/50], Loss: 0.0699\n",
      "Epoch [47/50], Loss: 0.0669\n",
      "Epoch [48/50], Loss: 0.0695\n",
      "Epoch [49/50], Loss: 0.0666\n",
      "Epoch [50/50], Loss: 0.0692\n"
     ]
    }
   ],
   "source": [
    "# Truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "\n",
    "    for i in range(0, input_tensor.size(1) - timesteps, timesteps):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = input_tensor[:, i:i+timesteps].to(device)\n",
    "        targets = input_tensor[:, (i+1):(i+1)+timesteps].to(device)\n",
    "        \n",
    "        states = detach(states)\n",
    "        outputs,_ = model(inputs, states)\n",
    "        loss = loss_fn(outputs, targets.reshape(-1))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        # Perform Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "    else:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'\n",
    "           .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly selected word index  1635\n",
      "\n",
      "[Enter Lady Bracknell. [Sternly.] Mr. Worthing, \n",
      "Jack. Darling! International letters to me if any kind. \n",
      "Gwendolen. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    random_sentences = ''\n",
    "\n",
    "    # Set intial hidden ane cell states\n",
    "    state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "             torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "    # Select one word index randomly and convert it to shape (1,1)\n",
    "    input = torch.randint(0,vocab_size, (1,)).long().unsqueeze(1).to(device)\n",
    "    print (\"randomly selected word index \", input.item())\n",
    "\n",
    "    for i in range(20):\n",
    "        output, _ = model(input, state)\n",
    "       \n",
    "        # we want our output to be \n",
    "        prob = output.exp()\n",
    "    \n",
    "        word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "\n",
    "        \n",
    "        # Replace the input with sampled word id for the next time step\n",
    "        input.fill_(word_id)\n",
    "    \n",
    "        word = corpus.dictionary.idx2word[word_id]\n",
    "        word = '\\n' if word == '<eos>' else word + ' '\n",
    "        random_sentences += word\n",
    "    print (random_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
